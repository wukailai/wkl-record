MQ作用：系统解偶、异步通知、流量削峰；

RabbitMQ：
    队列模型和发布-订阅模型最大的区别就是：一份消息数据能不能被消费多次；
    现代消息队列产品大多使用发布-订阅的消息模型，但RabbitMQ是个例外，依然坚持使用队列消息模型；
    RabbitMQ解决多端消费的方式：Exchange位于生产者和队列之间，生产者直接将消息发送给Exchange，由Exchange上配置的策略决定将消息投递到哪个队列；
    几乎所有的消息队列产品都使用一种非常常用的“请求-确认”机制，确保消息不会在传递过程中因为网络或服务器故障而丢失；

RocketMQ：
    RocketMQ“请求-确认”机制：
        1）生产端生产者先将消息发送给服务端，也就是Broker，服务端在收到消息并将消息写入主题或队列后，会给生产者发送确认响应。如果生产者没有收到服务端的确认响应或者收到了失败的响应，则会重新发送消息；
        2）在消费端，消费者收到消息并完成自己的业务逻辑后，也会给服务端发送消费成功的确认消息，服务端只有收到消费确认后，才认为一条消息被成功消费，否则他会给消费者重新发送这条消息；
    RocketMQ确认机制带来的问题：为了确保消息的有序性，在某一条消息被成功消费之前，下一条消息是不能被消息的，否则就违背了消息的有序性原则，每个主题在任意时刻只能有一个消息者实例在进行消费，
    也就无法通过水平扩展消费者实例个数来提高消费端整体的消费性能，因此RocketMQ引入了队列的概念；
    RocketMQ队列：每个主题中包含多个队列，通过多个队列来实现并行的生产和消费（所以RocketMQ只能队列上保证消息的有序性，无法在主题上保证有序性）；
    RocketMQ订阅者的概念是通过消费组来体现的，每个消费组都消费主题中一份完整的数据，每个消费者负责消费组内的一部分消息，如果一条消息被某个消费者消费了，同一组内的其他消费者就不会再收到这条消息；
    RocketMQ上消费过的消息不会被立即删除，取而代之的是每个队列上维护一个消费位置（consumer offset，Talos叫commit offset），丢消息大多是消费位置处理不当造成的；

Kafka：
    概念同RocketMQ，只不过RocketMQ中的队列在kafka里面叫分区；
    offset可能是消费者批量处理后才提交到zk，重启后再消费时就可能会收到重复消息；
    生产消息阶段处理消息丢失方式：消息发送后，回调消息发送成功或者失败的接口。那么业务层面也就可以根据是否发送成功和失败做处理，
    比如发送前缓存到redis，发送成功后从redis中移除，对于在redis中一直没有处理的，再进行重发操作；
    消费消息阶段处理消息丢失方式：取消autoACK，业务逻辑处理完后手动ACK；
    解决消息积压的方式：修复consumer故障（如果有的话），新建topic，增加分区数量，消息写入新的topic，消费新topic的consumer的数量也应该增加；
    分区设置太大的弊端：占内存，占文件句柄，broker一旦挂掉，增加选举耗时；
    消息的状态由Consumer控制，即offset值由consumer来维护；
    kafka副本指的是partition副本，并且只有一个partition会被选择为leader；
    kafka所有的读写操作均由leader处理，跟consumer一样，所有follower都从leader那里拉取日志；
    kafka保证数据可靠性：
        1）分区数据备份，数据同步；
        2）ISR（ISR中的所有follower都与leader保持高度一致）：同步副本，和leader保持同步的follower动态集合（如果follower（主动从leader拉数据）长时间未向leader同步数据，踢出ISR，ISR也可以作为选举leader的候选集合）。当ISR中的follower完成数据的同步之后，leader就会给生产者发送ack；
        3）ACK机制：producer收到ack（ack=0：生产者消息发出去就算成功，ack=1：Leader ack，ack=all：leader+follower全部ack），就会进行下一轮的发送，否则重新发送数据；
    producer处理此类故障所采取的提交策略类型：
        1）at-least-once（重复的消息重试写入kafka）；
        2）at-most-once（不重试，消息丢失）；
        3）exactly-once（最理想，采用上面的redis方案）；
    kafka Rebalance：给消费组分配分区的过程，因此触发Rebalance的事件有：
        1）消费组变动，比如有新消费者加入或者有消费者离开组；
        2）消费组订阅的Topic变动；
        3）消费组订阅Topic的分区发生了变化；
    producer将消息推送到broker，consumer从broker拉取消息（consumer消费速率不确定，避免consumer崩溃）；
    kafka消息丢失的情况：
        1）消费端自动提交offersets设置为true，分区offset更新，消费者挂了。解决方案：consumer手动提交，但是有可能会引入重复消费的问题；
        2）尚未写入磁盘（异步批量化写入磁盘），数据丢失。生产者重试机制，设置合理的重试间隔；
        3）磁盘数据丢失。解决方案：多个副本保证数据完整性；
        4）生产端消费丢失：调用producer.send(msg, callback)，callback删除redis缓存+重试机制，重试间隔不要太短；
        5）调参解决：发送成功判定ack=all、replication.factor=n指定多个副本写入成功、ack=1，leader写入成功即可、ack=0，消息发出去即可；
    kafka 0.10版本以后，生产者把消息先写入缓存，另起Sender线程从缓存里面批量取消息（可设置大小，如果一直达不到设置大小，定时发送），然后批量发送；
    Kafka判断一个节点是否活着有两个条件：
        1）节点必须可以维护和ZooKeeper的连接，Zookeeper通过心跳机制检查每个节点的连接；
        2）如果节点是个follower,他必须能及时的同步leader的写操作，延时不能太久；
    Leader节点会追踪所有同步节点状态，一旦延时太久，Leader就会把节点移除；
    Kafka延迟队列（延迟放在消费端）：
        1）另外创建一个延迟Topic，延迟消息都发到延迟Topic里；
        2）有专门的服务来消费延迟Topic的消息，取到消息之后存储起来，定期检查消息是否已经延迟时间；
        3）已到延迟时间的消息，重新发送到原先Topic；
    kafka replica物理上由无数个固定大小的段文件组成，每个段文件包含一个数据文件和索引文件，索引文件只有两个字段：relativeOffset（日志索引的相对偏移量）、position（消息存储在磁盘的物理位置）；
    kafka追溯消息两个维度：
        1）基于时间点的消息回溯，需要broker收到消息之后为每条消息添加时间戳：根据时间戳查找offset，根据offset进行消息的回溯功能；
        2）基于消息偏移量回溯，重置offset即可；